{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cora'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "from kcenter import make_all_dists, greedy_kcenter, gonzalez_kcenter, CenterObjective, make_dists_igraph, rounding\n",
    "from models import GCNLink, GCNClusterNet, GCNDeep, GCNDeepSigmoid, GCN\n",
    "from utils import make_normalized_adj, negative_sample, edge_dropout, load_nofeatures, accuracy, calculate_accuracy\n",
    "from modularity import baseline_spectral, partition, greedy_modularity_communities, make_modularity_matrix\n",
    "from loss_functions import loss_kcenter, loss_modularity\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_features = True\n",
    "# For some reason the original code provides its own load_data function but uses the\n",
    "# function from pygcn, which makes some feature normaliztion with sparse matrix.\n",
    "if(normalize_features):\n",
    "    from pygcn import load_data\n",
    "else:\n",
    "    from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Object(object):\n",
    "#     pass\n",
    "\n",
    "# args = Object()\n",
    "\n",
    "class AttrDict(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "no_cuda = True\n",
    "\n",
    "args = AttrDict(dict(\n",
    "    dataset = dataset_name,\n",
    "    train_pct = 0.40,\n",
    "    objective = 'modularity',\n",
    "    hidden = 50,\n",
    "    embed_dim = 50,\n",
    "    dropout = 0.2,\n",
    "    edge_dropout = 0.2,\n",
    "    K = 7,\n",
    "    train_iters = 1001,\n",
    "    lr = 0.001,\n",
    "    num_cluster_iter = 1,\n",
    "    weight_decay = 5e-4,\n",
    "    pure_opt = True,\n",
    "    clustertemp = 70,\n",
    "    kcentertemp = 100,\n",
    "    kcentermintemp = 0,\n",
    "    seed = 24,\n",
    "    negsamplerate = 1,\n",
    "    cuda = not no_cuda and torch.cuda.is_available(),\n",
    "    use_igraph = True,\n",
    "))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "run_decision = True\n",
    "run_ts = True\n",
    "run_gcne2e = True\n",
    "run_train_only = True\n",
    "calculate_opt = True\n",
    "    \n",
    "pure_opt = args.pure_opt\n",
    "test_cluster_auc = False\n",
    "\n",
    "reload_data = True\n",
    "make_objectives = False\n",
    "if reload_data:\n",
    "    make_objectives = True\n",
    "\n",
    "has_features = True\n",
    "\n",
    "train_pct = args.train_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora_test_0.40 dataset...\n",
      "Loading cora_valid_0.40 dataset...\n",
      "Loading cora_train_0.40 dataset...\n",
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "if reload_data:\n",
    "    if has_features:\n",
    "        adj_test, features_test, labels, idx_train, idx_val, idx_test = load_data('data/{}/'.format(args.dataset), '{}_test_{:.2f}'.format(args.dataset, train_pct))\n",
    "        adj_valid, features_valid, labels, idx_train, idx_val, idx_test = load_data('data/{}/'.format(args.dataset), '{}_valid_{:.2f}'.format(args.dataset, train_pct))\n",
    "        adj_train, features_train, labels, idx_train, idx_val, idx_test = load_data('data/{}/'.format(args.dataset), '{}_train_{:.2f}'.format(args.dataset, train_pct))\n",
    "    else:\n",
    "        adj_all, features, labels = load_nofeatures(args.dataset, '')\n",
    "        features_train = features\n",
    "        features_test = features\n",
    "        n = adj_all.shape[0]\n",
    "        adj_train, features, labels = load_nofeatures(args.dataset, '_train_{:.2f}'.format(train_pct), n)\n",
    "        adj_test, features, labels = load_nofeatures(args.dataset, '_test_{:.2f}'.format(train_pct), n)\n",
    "        adj_valid, features, labels = load_nofeatures(args.dataset, '_valid_{:.2f}'.format(train_pct), n)\n",
    "\n",
    "\n",
    "adj_test = adj_test.coalesce()\n",
    "adj_valid = adj_valid.coalesce()\n",
    "adj_train = adj_train.coalesce()\n",
    "n = adj_train.shape[0]\n",
    "K = args.K\n",
    "bin_adj_test = (adj_test.to_dense() > 0).float()\n",
    "bin_adj_train = (adj_train.to_dense() > 0).float()\n",
    "m_train = bin_adj_train.sum()\n",
    "bin_adj_valid = (adj_valid.to_dense() > 0).float()\n",
    "bin_adj_all = (bin_adj_train + bin_adj_test + bin_adj_valid > 0).float()\n",
    "adj_all = make_normalized_adj(bin_adj_all.nonzero(), n)\n",
    "nfeat = features_test.shape[1]\n",
    "\n",
    "adj_all, features_test, labels, idx_train, idx_val, idx_test = load_data('data/{}/'.format(args.dataset), '{}'.format(args.dataset))\n",
    "adj_all = adj_all.coalesce()\n",
    "adj_test = adj_all\n",
    "bin_adj_all = (adj_all.to_dense() > 0).float()\n",
    "n = adj_all.shape[0]\n",
    "K= args.K\n",
    "nfeat = features_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_if_possible(r=None, print_normalized=False):\n",
    "    if features_train.shape[1] == 2:\n",
    "        if normalize_features and not print_normalized:\n",
    "            x = []\n",
    "            y = []\n",
    "            with open(\"data/%s/%s.content\" % (args.dataset, args.dataset)) as ss:\n",
    "                for line in ss:\n",
    "                    _, xx, yy, _ = line.split(' ')\n",
    "                    x.append(float(xx))\n",
    "                    y.append(float(yy))\n",
    "        else:\n",
    "            x = features_train[:,0]\n",
    "            y = features_train[:,1]\n",
    "        \n",
    "        if print_normalized:\n",
    "            text = \" normalized\"\n",
    "        else:\n",
    "            text = \"\"\n",
    "        if r is None:\n",
    "            plt.scatter(x, y, c=labels, s=5)\n",
    "            plt.title(\"The%s dataset with true labels\" % text)\n",
    "        else:\n",
    "            predictions = r.argmax(dim=1)\n",
    "            plt.scatter(x, y, c=predictions, s=5)\n",
    "            plt.title(\"The%s dataset with predicted labels\" % text)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_if_possible()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if normalize_features:\n",
    "    plot_if_possible(print_normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_ts = GCNLink(nfeat=nfeat,\n",
    "            nhid=args.hidden,\n",
    "            nout=args.embed_dim,\n",
    "            dropout=args.dropout)\n",
    "\n",
    "model_cluster = GCNClusterNet(nfeat=nfeat,\n",
    "            nhid=args.hidden,\n",
    "            nout=args.embed_dim,\n",
    "            dropout=args.dropout,\n",
    "            K = args.K,\n",
    "            cluster_temp = args.clustertemp)\n",
    "\n",
    "#keep a couple of initializations here so that the random seeding lines up\n",
    "#with results reported in the paper -- removing these is essentially equivalent to \n",
    "#changing the seed\n",
    "_ = GCN(nfeat, args.hidden, args.embed_dim, args.dropout)\n",
    "_ = nn.Parameter(torch.rand(K, args.embed_dim))\n",
    "\n",
    "#uses GCNs to predict the cluster membership of each node\n",
    "model_gcn = GCNDeep(nfeat=nfeat,\n",
    "            nhid=args.hidden,\n",
    "            nout=args.K,\n",
    "            dropout=args.dropout,\n",
    "            nlayers=2)\n",
    "\n",
    "#uses GCNs to predict the probability that each node appears in the solution\n",
    "model_gcn_x = GCNDeepSigmoid(nfeat=nfeat,\n",
    "            nhid=args.hidden,\n",
    "            nout=1,\n",
    "            dropout=args.dropout,\n",
    "            nlayers=2)\n",
    "\n",
    "if args.objective == 'kcenter':\n",
    "    model_gcn = model_gcn_x\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model_cluster.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model_cluster.cuda()\n",
    "    model_ts.cuda()\n",
    "    features = features.cuda()\n",
    "    adj_train = adj_train.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "losses = []\n",
    "losses_test = []\n",
    "num_cluster_iter = args.num_cluster_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxilary data for objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if make_objectives:\n",
    "    if args.objective == 'kcenter':\n",
    "        try:\n",
    "            dist_all = torch.load('{}_test_dist.pt'.format(args.dataset))\n",
    "            dist_train = torch.load('{}_{}_train_dist.pt'.format(args.dataset, train_pct))\n",
    "            diameter = dist_all.max()\n",
    "        except:\n",
    "            dist_all = make_all_dists(bin_adj_all, 100)\n",
    "            diameter = dist_all[dist_all < 100].max()\n",
    "            dist_all[dist_all == 100] = diameter\n",
    "            torch.save(dist_all, '{}_test_dist.pt'.format(args.dataset))\n",
    "            dist_train = make_all_dists(bin_adj_train, 100)\n",
    "            dist_train[dist_train == 100] = diameter\n",
    "            torch.save(dist_train, '{}_{}_train_dist.pt'.format(args.dataset, train_pct))\n",
    "        obj_train = CenterObjective(dist_train, diameter, args.kcentermintemp)\n",
    "        obj_train_hardmax = CenterObjective(dist_train, diameter, args.kcentermintemp, hardmax=True)\n",
    "        obj_test = CenterObjective(dist_all, diameter, args.kcentertemp, hardmax=True)\n",
    "        obj_test_softmax = CenterObjective(dist_all, diameter, args.kcentermintemp)\n",
    "\n",
    "    if args.objective == 'modularity':\n",
    "        mod_train = make_modularity_matrix(bin_adj_train)\n",
    "        mod_test = make_modularity_matrix(bin_adj_test)\n",
    "        mod_valid = make_modularity_matrix(bin_adj_valid)\n",
    "        mod_all = make_modularity_matrix(bin_adj_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if args.objective == 'modularity':\n",
    "    loss_fn = loss_modularity\n",
    "    test_object = mod_all\n",
    "    train_object = mod_train\n",
    "    test_only_object = mod_test\n",
    "    valid_object = mod_valid\n",
    "elif args.objective == 'kcenter':\n",
    "    loss_fn = loss_kcenter\n",
    "    test_object= obj_test\n",
    "    train_object = obj_train\n",
    "    test_only_object = None\n",
    "    valid_object = None\n",
    "else:\n",
    "    raise Exception('unknown objective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision-Focused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterNet value 0.5756863355636597\n",
      "ClusterNet accuracy:  0.04689807976366322\n"
     ]
    }
   ],
   "source": [
    "best_train_val = 100\n",
    "if run_decision:\n",
    "    for t in range(args.train_iters):\n",
    "        #pure optimization setting: get loss with respect to the full graph\n",
    "        if pure_opt:\n",
    "            mu, r, embeds, dist = model_cluster(features_test, adj_all, num_cluster_iter)\n",
    "            loss = loss_fn(mu, r, embeds, dist, bin_adj_all, test_object, args)\n",
    "        #link prediction setting: get loss with respect to training edges only\n",
    "        else:\n",
    "            mu, r, embeds, dist = model_cluster(features_train, adj_train, num_cluster_iter)\n",
    "            loss = loss_fn(mu, r, embeds, dist, bin_adj_train, train_object, args)\n",
    "        if args.objective != 'kcenter':\n",
    "            loss = -loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #increase number of clustering iterations after 500 updates to fine-tune\n",
    "        #solution\n",
    "        if t == 500:\n",
    "            num_cluster_iter = 5\n",
    "        #every 100 iterations, look and see if we've improved on the best training loss\n",
    "        #seen so far. Keep the solution with best training value.\n",
    "        if t % 100 == 0:\n",
    "            #round solution to discrete partitioning\n",
    "            if args.objective == 'modularity':\n",
    "                r = torch.softmax(100*r, dim=1)\n",
    "            #evalaute test loss -- note that the best solution is\n",
    "            #chosen with respect training loss. Here, we store the test loss\n",
    "            #of the currently best training solution\n",
    "            loss_test = loss_fn(mu, r, embeds, dist, bin_adj_all, test_object, args)\n",
    "            #for k-center problem, keep track of the fractional x with best\n",
    "            #training loss, to do rounding after\n",
    "            if loss.item() < best_train_val:\n",
    "                best_train_val = loss.item()\n",
    "                curr_test_loss = loss_test.item()\n",
    "                #convert distances into a feasible (fractional x)\n",
    "                x_best = torch.softmax(dist*args.kcentertemp, 0).sum(dim=1)\n",
    "                x_best = 2*(torch.sigmoid(4*x_best) - 0.5)\n",
    "                if x_best.sum() > K:\n",
    "                    x_best = K*x_best/x_best.sum()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "    #for k-center: round 50 times and take the solution with best training\n",
    "    #value\n",
    "    if args.objective == 'kcenter':\n",
    "        testvals = []; trainvals = []\n",
    "        for _ in range(50):\n",
    "            y = rounding(x_best)\n",
    "            testvals.append(obj_test(y).item())\n",
    "            trainvals.append(obj_train(y).item())\n",
    "        print('ClusterNet value', testvals[np.argmin(trainvals)])\n",
    "    if args.objective == 'modularity':\n",
    "            print('ClusterNet value', curr_test_loss)\n",
    "            tmp_training_state = model_cluster.training\n",
    "            model_cluster.training = False\n",
    "            mu, r, embeds, dist = model_cluster(features_train, adj_train, num_cluster_iter)\n",
    "            model_cluster.training = tmp_training_state\n",
    "            print(\"ClusterNet accuracy: \", accuracy(r, labels).item())\n",
    "            plot_if_possible(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two stage\n",
      "0 0.6976549029350281 0.6979550719261169 0.550937959604964\n",
      "10 0.6936492323875427 0.6939929723739624 0.5672309299976281\n",
      "20 0.692654550075531 0.6930332183837891 0.57672392672294\n",
      "30 0.6924092769622803 0.6927725076675415 0.6020536509390446\n",
      "40 0.6922897100448608 0.6926782727241516 0.6283998148939018\n",
      "50 0.6922233700752258 0.6926360726356506 0.6386608879063167\n",
      "60 0.6921097636222839 0.6926025152206421 0.6548568245846436\n",
      "70 0.6919981837272644 0.692530632019043 0.677261648615624\n",
      "80 0.6918414831161499 0.6924417018890381 0.6804755014070756\n",
      "90 0.6916250586509705 0.6923367381095886 0.687436373757532\n",
      "100 0.6913665533065796 0.6922526359558105 0.6693443875583673\n",
      "110 0.6910643577575684 0.6920222640037537 0.6686139163472129\n",
      "120 0.6904924511909485 0.6917594075202942 0.6634707941337172\n",
      "130 0.6898487210273743 0.6913962364196777 0.6592036922818925\n",
      "140 0.6889950633049011 0.6908406615257263 0.6703813520748181\n",
      "150 0.6875816583633423 0.6900725960731506 0.6670087718727855\n",
      "160 0.6858158707618713 0.689021110534668 0.6709068057093508\n",
      "170 0.6833244562149048 0.6876484155654907 0.6797646398561423\n",
      "180 0.67946857213974 0.6859713792800903 0.6731119716827867\n",
      "190 0.6738286018371582 0.6829913258552551 0.6813624292869969\n",
      "200 0.6676422953605652 0.6794937252998352 0.6790841818044907\n",
      "210 0.6562013030052185 0.6741011738777161 0.6819383195360142\n",
      "220 0.6444258093833923 0.6687995791435242 0.6830969264693899\n",
      "230 0.6292195320129395 0.6608126759529114 0.6821025973330316\n",
      "240 0.6080249547958374 0.6556117534637451 0.6806267771467724\n",
      "250 0.5873890519142151 0.6481946706771851 0.680156281716494\n",
      "260 0.5673273801803589 0.6411030292510986 0.6892300608520155\n",
      "270 0.5517717599868774 0.6402274370193481 0.6885195488645904\n",
      "280 0.5251360535621643 0.6404751539230347 0.6910462763199154\n",
      "290 0.5174332857131958 0.6431645750999451 0.692298637323006\n",
      "7\n",
      "agglomerative tensor(0.1841)\n",
      "agglomerative accuracy:  0.1362629246676514\n",
      "recursive tensor(0.0037)\n",
      "recursive accuracy:  0.12850812407680945\n",
      "spectral tensor(0.1213)\n",
      "spectral accuracy:  0.1451255539143279\n"
     ]
    }
   ],
   "source": [
    "def train_twostage(model_ts):\n",
    "    optimizer_ts = optim.Adam(model_ts.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "    edges = adj_train.indices().t()\n",
    "    edges_test = adj_test.indices().t()\n",
    "    edges_test_eval, labels_test_eval = negative_sample(edges_test, 1, bin_adj_train)\n",
    "#    print(edges_test_eval)\n",
    "    for t in range(300):\n",
    "        adj_input = make_normalized_adj(edge_dropout(edges, args.edge_dropout), n)\n",
    "        edges_eval, labels = negative_sample(edges, args.negsamplerate, bin_adj_train)\n",
    "        preds = model_ts(features_train, adj_input, edges_eval)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(preds, labels)\n",
    "        optimizer_ts.zero_grad()\n",
    "        loss.backward()\n",
    "        if t % 10 == 0:\n",
    "            preds_test_eval = model_ts(features_train, adj_input, edges_test_eval)\n",
    "            test_ce = torch.nn.BCEWithLogitsLoss()(preds_test_eval, labels_test_eval)\n",
    "            test_auc = sklearn.metrics.roc_auc_score(labels_test_eval.long().detach().numpy(), nn.Sigmoid()(preds_test_eval).detach().numpy())\n",
    "            print(t, loss.item(), test_ce.item(), test_auc)\n",
    "        optimizer_ts.step()\n",
    "\n",
    "if test_cluster_auc:\n",
    "    model_linkpred = GCNLink(nfeat=nfeat,\n",
    "            nhid=args.hidden,\n",
    "            nout=args.embed_dim,\n",
    "            dropout=args.dropout)\n",
    "    model_linkpred.GCN = model_cluster.GCN\n",
    "    model_linkpred.GCN.requires_grad = False\n",
    "    train_twostage(model_linkpred)\n",
    "\n",
    "\n",
    "calculate_ts_performance = False\n",
    "if run_ts:\n",
    "    print('two stage')\n",
    "    train_twostage(model_ts)\n",
    "    #predict probability that all unobserved edges exist\n",
    "    indices = torch.tensor(np.arange(n))\n",
    "    to_pred = torch.zeros(n**2, 2)\n",
    "    to_pred[:, 1] = indices.repeat(n)\n",
    "    for i in range(n):\n",
    "        to_pred[i*n:(i+1)*n, 0] = i\n",
    "    to_pred = to_pred.long()\n",
    "    preds = model_ts(features_train, adj_train, to_pred)\n",
    "    preds = nn.Sigmoid()(preds).view(n, n)\n",
    "\n",
    "    preds = bin_adj_train + (1 - bin_adj_train)*preds\n",
    "\n",
    "    if args.objective == 'modularity':\n",
    "        r = greedy_modularity_communities(preds, K)\n",
    "        print('agglomerative', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"agglomerative accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "        \n",
    "        r = partition(preds, K)\n",
    "        print('recursive', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"recursive accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "\n",
    "        degrees = preds.sum(dim=1)\n",
    "        preds = torch.diag(1./degrees)@preds\n",
    "        mod_pred = make_modularity_matrix(preds)\n",
    "        r = baseline_spectral(mod_pred, K)\n",
    "        print('spectral', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"spectral accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "    elif args.objective == 'kcenter':\n",
    "        try:\n",
    "            dist_ts = torch.load('{}_twostage_dist.pt'.format(args.dataset))\n",
    "            print('loaded ts dists from {}'.format('{}_twostage_dist.pt'.format(args.dataset)))\n",
    "        except:\n",
    "            print('making dists')\n",
    "            if args.use_igraph:\n",
    "                print('using igraph')\n",
    "                dist_ts =  make_dists_igraph(preds)\n",
    "            else:\n",
    "                print('using networkx')\n",
    "                dist_ts = make_all_dists(preds, 100)\n",
    "                diameter = dist_ts[dist_ts < 100].max()\n",
    "                dist_ts[dist_ts == 100] = diameter\n",
    "            print('made dists')\n",
    "            torch.save(dist_ts, '{}_twostage_dist.pt'.format(args.dataset))\n",
    "        dist_ts = dist_ts.float()\n",
    "        diameter = dist_ts.max()\n",
    "        x = gonzalez_kcenter(dist_ts, K)\n",
    "        print('gonzalez ts', obj_train_hardmax(x), obj_test(x))\n",
    "        print(dist_ts.type(), diameter.type())\n",
    "        x = greedy_kcenter(dist_ts, diameter, K)\n",
    "        print('greedy ts', obj_train_hardmax(x), obj_test(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2E GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just GCN\n",
      "0 -0.0003772028721868992 -1.2361405197225395e-06 1.1375951913805693e-07\n",
      "100 -0.0018916751723736525 0.003027205355465412 0.003719148226082325\n",
      "200 -0.00015944383630994707 0.0003794107760768384 0.0007440358749590814\n",
      "300 -0.00020667437638621777 0.00037874022382311523 0.0007424590876325965\n",
      "400 -0.00026252202223986387 0.001134658814407885 0.0022295506205409765\n",
      "500 -0.00043619528878480196 0.00037388905184343457 0.0007375568384304643\n",
      "600 -0.0010287465993314981 0.0003778848913498223 -1.218813508785388e-06\n",
      "700 -0.0010210201144218445 0.0030236588791012764 0.0037156138569116592\n",
      "800 -0.00028532487340271473 0.0006094782147556543 0.0011970620835199952\n",
      "900 -0.0006831673672422767 6.367894457071088e-06 1.2431093637133017e-05\n",
      "train min 0.003027205355465412\n",
      "e2e gcn accuracy:  0.15472673559822747\n"
     ]
    }
   ],
   "source": [
    "if run_gcne2e:\n",
    "    print('just GCN')\n",
    "    optimizer_gcn = optim.Adam(model_gcn.parameters(), lr = args.lr,\n",
    "                               weight_decay = args.weight_decay)\n",
    "    if args.objective == 'modularity':\n",
    "        best_train_val = 0\n",
    "    if args.objective == 'kcenter':\n",
    "        best_train_val = 100\n",
    "\n",
    "    for t in range(1000):\n",
    "        best_train_loss = 100\n",
    "        if pure_opt:\n",
    "            if args.objective == 'modularity' or args.objective == 'maxcut':\n",
    "                r = model_gcn(features_test, adj_all)\n",
    "                r = torch.softmax(args.clustertemp*r, dim = 1)\n",
    "                loss = -loss_fn(None, r, None, None, bin_adj_train, train_object, args)\n",
    "            elif args.objective == 'kcenter' or args.objecive == 'influmax':\n",
    "                x = model_gcn(features_test, adj_all)\n",
    "                if x.sum() > K:\n",
    "                    x = K*x/x.sum()\n",
    "                loss = -test_object(x)\n",
    "        else:\n",
    "            if args.objective == 'modularity' or args.objective == 'maxcut':\n",
    "                r = model_gcn(features_train, adj_train)\n",
    "                r = torch.softmax(r, dim = 1)\n",
    "                loss = -loss_fn(None, r, None, None, bin_adj_train, train_object, args)\n",
    "            elif args.objective == 'kcenter' or args.objecive == 'influmax':\n",
    "                x = model_gcn(features_train, adj_train)\n",
    "                if x.sum() > K:\n",
    "                    x = K*x/x.sum()\n",
    "                loss = -train_object(x)\n",
    "        if args.objective == 'kcenter':\n",
    "            loss = -loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if t % 100 == 0:\n",
    "            if args.objective == 'modularity' or args.objective == 'maxcut':\n",
    "                r = torch.softmax(100*r, dim=1)\n",
    "                loss_test = loss_fn(None, r, None, None, bin_adj_all, test_object, args)\n",
    "                loss_test_only = loss_fn(None, r, None, None, bin_adj_test, test_only_object, args)\n",
    "            elif args.objective == 'kcenter' or args.objecive == 'influmax':\n",
    "                loss_test = -test_object(x)\n",
    "                loss_test_only = torch.tensor(0).float()\n",
    "            losses_test.append(loss_test.item())\n",
    "            print(t, loss.item(), loss_test.item(), loss_test_only.item())\n",
    "            if loss.item() < best_train_val:\n",
    "                curr_test_loss = loss_test.item()\n",
    "                best_train_val = loss.item()\n",
    "                if args.objective == 'kcenter' or args.objective == 'influmax':\n",
    "                    x_best = x\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "    if args.objective == 'kcenter':\n",
    "        from influmax import rounding\n",
    "        testvals = []; trainvals = []; trainvalshardmax = []\n",
    "        for _ in range(50):\n",
    "            y = rounding(x_best)\n",
    "            testvals.append(obj_test(y).item())\n",
    "            trainvals.append(obj_train(y).item())\n",
    "            trainvalshardmax.append(obj_train_hardmax(y).item())\n",
    "        print('train min', testvals[np.argmin(trainvals)])\n",
    "        print('hardmax train min', testvals[np.argmin(trainvalshardmax)])\n",
    "        print('absolute min', min(testvals))\n",
    "    if args.objective == 'modularity':\n",
    "        print('train min', curr_test_loss)\n",
    "        print(\"e2e gcn accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-only baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "agglomerative tensor(0.2021)\n",
      "agglomerative accuracy:  0.14697193500738553\n",
      "recursive tensor(0.0835)\n",
      "recursive accuracy:  0.0672082717872969\n",
      "spectral tensor(0.0357)\n",
      "spectral accuracy:  0.06610044313146234\n"
     ]
    }
   ],
   "source": [
    "if run_train_only:\n",
    "    if args.objective == 'modularity':\n",
    "        preds = bin_adj_train\n",
    "        r = greedy_modularity_communities(preds, K)\n",
    "        print('agglomerative', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"agglomerative accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "\n",
    "        r = partition(preds, K)\n",
    "        print('recursive', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"recursive accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "        \n",
    "        degrees = preds.sum(dim=1)\n",
    "        preds = torch.diag(1./degrees)@preds\n",
    "        mod_pred = make_modularity_matrix(preds)\n",
    "        r = baseline_spectral(mod_pred, K)\n",
    "        print('spectral', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"spectral accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "    elif args.objective == 'kcenter':\n",
    "        x = gonzalez_kcenter(dist_train, K)\n",
    "        print('gonzalez train', obj_test(x))\n",
    "        x = greedy_kcenter(dist_train, diameter, K)\n",
    "        print('greedy train', obj_test(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline optimization algorithms on full graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "agglomerative tensor(0.0774)\n",
      "agglomerative accuracy:  0.09601181683899557\n",
      "recursive tensor(0.1977)\n",
      "recursive accuracy:  0.1883308714918759\n",
      "spectral tensor(0.1853)\n",
      "spectral accuracy:  0.07533234859675036\n"
     ]
    }
   ],
   "source": [
    "if calculate_opt:\n",
    "    if args.objective == 'modularity':\n",
    "        preds = bin_adj_all\n",
    "        r = greedy_modularity_communities(preds, K)\n",
    "        print('agglomerative', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"agglomerative accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "        \n",
    "        r = partition(preds, K)\n",
    "        print('recursive', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"recursive accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "\n",
    "        degrees = preds.sum(dim=1)\n",
    "        preds = torch.diag(1./degrees)@preds\n",
    "        mod_pred = make_modularity_matrix(preds)\n",
    "        r = baseline_spectral(mod_pred, K)\n",
    "        print('spectral', loss_fn(None, r, None, None, bin_adj_all, test_object, args))\n",
    "        print(\"spectral accuracy: \", accuracy(r, labels).item())\n",
    "        plot_if_possible(r)\n",
    "    elif args.objective == 'kcenter':\n",
    "        x = gonzalez_kcenter(dist_all, K)\n",
    "        print('gonzalez all', obj_test(x))\n",
    "        x = greedy_kcenter(dist_all, diameter, K)\n",
    "        print('greedy all', obj_test(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
